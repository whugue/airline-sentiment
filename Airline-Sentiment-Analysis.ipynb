{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Fletcher Airline Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import pylab\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define Preprocessing and Tokenizer Functions to Read into Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = re.sub(r\"(?:\\https?\\://)\\S+\", \"\", text) #remove urls\n",
    "    text = re.sub('\\@(\\w+)', \" \", text).replace(\": \",\"\") #remove usernames\n",
    "    text = re.sub('#(\\w+)', \" \", text) #remove hashtags\n",
    "    text = text.replace(\"RT \",\"\") #remove RT Symbols\n",
    "    text = text.replace(\"RT: \",\"\") #remove RT Symbols\n",
    "    text = re.sub(\"[^a-zA-Z,]+\", \" \", text) #remove other non-alpha characters\n",
    "    text = text.strip(\" \") #remove leading and trailing whitespace\n",
    "    \n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)  \n",
    "    \n",
    "    return [x for x in tokenizer.tokenize(preprocess(text)) if len(x)>=4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read-In NLTK Twitter Sentiment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_demo_tweets():\n",
    "    tweets = []\n",
    "    for tweet in twitter_samples.strings(\"negative_tweets.json\"):\n",
    "        tweets.append({\"text\":tweet, \"sentiment\":1})\n",
    "        \n",
    "    for tweet in twitter_samples.strings(\"positive_tweets.json\"):\n",
    "        tweets.append({\"text\":tweet, \"sentiment\":0})\n",
    "        \n",
    "    return pd.DataFrame(tweets)\n",
    "\n",
    "\n",
    "demo_tweets=read_demo_tweets()\n",
    "print demo_tweets.sentiment.value_counts(dropna=False)\n",
    "demo_tweets.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in CrowdFlower Data. Split Unambigous Tweets into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cf = pd.read_pickle(\"data/crowdflower/airline_sentiment_with_tb_vader.pkl\")\n",
    "cf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print cf.text_blob_sentiment.describe()\n",
    "print cf.vader_sentiment.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cf_unambig = cf.ix[(cf[\"_trusted_judgments\"]>2) & (cf[\"airline_sentiment:confidence\"]==1.00)]\n",
    "cf_ambig = cf.ix[(cf[\"_trusted_judgments\"]>2) & (cf[\"airline_sentiment:confidence\"]<1.00)]\n",
    "\n",
    "cf_train, cf_test = train_test_split(cf_unambig, test_size=0.3, random_state=4444)\n",
    "\n",
    "print cf_ambig.shape, cf_unambig.shape, cf_train.shape, cf_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph Positive/ Neutral vs. Negative Ratings by Airline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "negative = cf_unambig.ix[cf_unambig.sentiment==1].groupby(by=\"airline\", as_index=False)[\"_unit_id\"].count()\n",
    "positive = cf_unambig.ix[cf_unambig.sentiment==0].groupby(by=\"airline\", as_index=False)[\"_unit_id\"].count()\n",
    "\n",
    "negative.rename(columns={\"_unit_id\":\"negative_tweets\"}, inplace=True)\n",
    "positive.rename(columns={\"_unit_id\":\"positive_tweets\"}, inplace=True)\n",
    "total = pd.merge(negative, positive, on=\"airline\")\n",
    "total[\"total_tweets\"] = total.negative_tweets + total.positive_tweets\n",
    "total.sort_values(by=\"total_tweets\", ascending=True, inplace=True)\n",
    "print total.airline.tolist()\n",
    "\n",
    "blue = sns.color_palette(\"deep\")[0]\n",
    "red = sns.color_palette(\"deep\")[2]\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "p1=plt.barh(np.arange(6), total.negative_tweets, 0.8, color=red, label=\"Negative\")\n",
    "p2=plt.barh(np.arange(6), total.positive_tweets, 0.8, color=blue, left=total.negative_tweets,label=\"Positive/ Neutral\")\n",
    "\n",
    "#plt.title(\"Tweets by Sentiment and Airline\", fontsize=32)\n",
    "plt.xlabel(\"Number of Tweets (Feb 2015)\", fontsize=26)\n",
    "#plt.yticks([(x+(0.75/2)) for x in range(0,6)],[])\n",
    "plt.tick_params(axis=\"x\", labelsize=24)\n",
    "plt.tick_params(axis=\"y\", colors=\"white\")\n",
    "plt.legend(loc=\"best\", fontsize=26)\n",
    "plt.savefig(\"graphics/twees_by_sentiment_airline.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search to Optimize Lexicon-Based Sentiment Analyzers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lexicon_grid_search(variable, test, step):\n",
    "    max_accuracy = 0\n",
    "    max_param = np.nan\n",
    "    \n",
    "    for x in np.arange(-1, 1, step):\n",
    "        pred_y = []\n",
    "        \n",
    "        for each in test[variable]:\n",
    "            if each < x:\n",
    "                pred_y.append(1) #negative\n",
    "            else:\n",
    "                pred_y.append(0) #positive/ neutral\n",
    "            \n",
    "        true_y = test[\"sentiment\"]\n",
    "        accuracy = accuracy_score(true_y, pred_y)\n",
    "            \n",
    "        if accuracy > max_accuracy:\n",
    "            max_accuracy = accuracy\n",
    "            max_param = x\n",
    "            \n",
    "    return (max_param, max_accuracy)\n",
    "\n",
    "print \"Text Blob 0.5: \"+str(lexicon_grid_search(\"text_blob_sentiment\", cf_test, 0.5))\n",
    "print \"Text Blob 0.1: \"+str(lexicon_grid_search(\"text_blob_sentiment\", cf_test, 0.1))\n",
    "print \"Text Blob 0.05: \"+str(lexicon_grid_search(\"text_blob_sentiment\", cf_test, 0.05))\n",
    "print \"\"\n",
    "print \"VADER 0.5: \"+str(lexicon_grid_search(\"vader_sentiment\", cf_test, 0.5))\n",
    "print \"VADER 0.1: \"+str(lexicon_grid_search(\"vader_sentiment\", cf_test, 0.1))\n",
    "print \"VADER 0.05: \"+str(lexicon_grid_search(\"vader_sentiment\", cf_test, 0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal TextBlob Sentiment Classifer attains a 74% accuracy on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Naive Bayes and SVM Models Trained on NLTK and Crowdflower Corpuses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "air_travel_sws = ENGLISH_STOP_WORDS.union([\"aa\",\"united\",\"usairways\",\"americanair\",\"southwestair\",\"jetblue\",\"http\",\\\n",
    "                                           \"virginamerica\",\"amp\",\"flight\",\"flights\",\"plane\",\"gate\",\"flightled\",\\\n",
    "                                           \"bag\",\"airline\",\"airport\",\"fly\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_count_model(classifier, train, test, sws, binary):\n",
    "    vectorizer = CountVectorizer(decode_error=\"ignore\", ngram_range=(1,1), min_df=2, max_df=0.2, stop_words=sws,\\\n",
    "                                 tokenizer=tokenize, binary=binary)\n",
    "    \n",
    "    vectorizer.fit(train[\"text\"])\n",
    "    \n",
    "    train_X = vectorizer.transform(train[\"text\"])\n",
    "    test_X = vectorizer.transform(test[\"text\"])\n",
    "    \n",
    "    train_y = train[\"sentiment\"]\n",
    "    test_y = test[\"sentiment\"]\n",
    "    \n",
    "    clf = classifier.fit(train_X, train_y)\n",
    "    \n",
    "    #print \"Confusion Matrix: \"\n",
    "    #print confusion_matrix(test_y, clf.predict(test_X))\n",
    "    \n",
    "    accuracy = accuracy_score(test_y, clf.predict(test_X))\n",
    "    features = zip(vectorizer.get_feature_names(), clf.coef_[0])\n",
    "    top10_features = sorted(features, reverse=True, key=lambda x: x[1])[:10]\n",
    "    \n",
    "    print \"Accuracy: \"+str(accuracy)\n",
    "    \n",
    "    return [accuracy, [x[0] for x in top10_features]]\n",
    "    \n",
    "\n",
    "def fit_tfidf_model(classifier, train, test, sws, binary):\n",
    "    vectorizer = TfidfVectorizer(decode_error=\"ignore\", ngram_range=(1,1), min_df=2, max_df=0.2, stop_words=sws,\n",
    "                                 tokenizer=tokenize, binary=binary)\n",
    "        \n",
    "    vectorizer.fit(train[\"text\"])\n",
    "    \n",
    "    train_X = vectorizer.transform(train[\"text\"])\n",
    "    test_X = vectorizer.transform(test[\"text\"])\n",
    "    \n",
    "    train_y = train[\"sentiment\"]\n",
    "    test_y = test[\"sentiment\"]\n",
    "    \n",
    "    clf = classifier.fit(train_X, train_y)\n",
    "    \n",
    "    #print \"Confusion Matrix: \"\n",
    "    #print confusion_matrix(test_y, clf.predict(test_X))\n",
    "    \n",
    "    accuracy = accuracy_score(test_y, clf.predict(test_X))\n",
    "    features = zip(vectorizer.get_feature_names(), clf.coef_[0])\n",
    "    top10_features=sorted(features, reverse=True, key=lambda x: x[1])[:10]\n",
    "    \n",
    "    print \"Accuracy: \"+str(accuracy)\n",
    "    \n",
    "    return [accuracy, [x[0] for x in top10_features]]\n",
    "\n",
    "\n",
    "def test_models(function, train, sws, label):\n",
    "    m1=function(BernoulliNB(binarize=None), train, cf_test, sws, True)\n",
    "    m2=function(MultinomialNB(), train, cf_test, sws, False)\n",
    "    m3=function(LinearSVC(), train, cf_test, sws, False)\n",
    "    \n",
    "    data = [{\"clf\":\"Bernoulli NB\", \"accuracy\":m1[0]*100, \"features\":m1[1], \"label\":label},\\\n",
    "            {\"clf\":\"Multinomial NB\", \"accuracy\":m2[0]*100, \"features\":m2[1], \"label\":label},\\\n",
    "            {\"clf\":\"Linear SVM\",\"accuracy\":m3[0]*100, \"features\":m3[1], \"label\":label}]\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "general_count = test_models(fit_count_model, demo_tweets, \"english\", \"General Tweets \\n(Count Vectorized)\")\n",
    "print \"\"\n",
    "general_tfidf = test_models(fit_tfidf_model, demo_tweets, \"english\", \"General Tweets \\n(TF-IDF Vectorized)\")\n",
    "print \"\"\n",
    "airline_count = test_models(fit_count_model, cf_train, air_travel_sws, \"Airline Tweets \\n(Count Vectorized)\")\n",
    "print \"\"\n",
    "airline_tfidf = test_models(fit_tfidf_model, cf_train, air_travel_sws, \"Airline Tweets \\n(TF-IDF Vectorized)\")\n",
    "print \"\"\n",
    "\n",
    "results = pd.concat([general_count, general_tfidf, airline_count, airline_tfidf], axis=0)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Accuracy Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "reds = sns.color_palette(\"Greens\")[2:5]\n",
    "blues = sns.color_palette(\"Blues\")[2:5]\n",
    "\n",
    "l1=plt.axhline(y=float(cf_test.sentiment.value_counts()[1]*100)/ cf_test.shape[0], color=\"black\")\n",
    "plt.axhline(y=lexicon_grid_search(\"text_blob_sentiment\", cf_test, 0.05)[1]*100, color=\"#ff00bf\")\n",
    "plt.axhline(y=lexicon_grid_search(\"vader_sentiment\", cf_test, 0.05)[1]*100, color=\"#8000ff\")\n",
    "\n",
    "sns.barplot(x=\"label\", y=\"accuracy\", hue=\"clf\", palette=blues, data=results)\n",
    "\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"Accuracy (%)\", fontsize=32)\n",
    "\n",
    "plt.tick_params(axis=\"x\", labelsize=28)\n",
    "plt.tick_params(axis=\"y\", labelsize=24)\n",
    "\n",
    "plt.legend(loc=\"upper left\", fontsize=28)\n",
    "\n",
    "pylab.ylim([50,100])\n",
    "plt.savefig(\"graphics/sentiment_analyzer_accuracy.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at Most Important Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_top_unigrams(df, clf, label):\n",
    "    print clf, label\n",
    "    \n",
    "    for each in df.ix[(df[\"clf\"]==clf) & (df[\"label\"]==label)][\"features\"].tolist()[0]:\n",
    "        print each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_top_unigrams(results, \"Bernoulli NB\", \"General Tweets \\n(Count Vectorized)\")\n",
    "print_top_unigrams(results, \"Bernoulli NB\", \"Airline Tweets \\n(Count Vectorized)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See if Upsampling General Tweets Makes any Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def upsampler(df, n):\n",
    "    return df.sample(n=n, replace=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
